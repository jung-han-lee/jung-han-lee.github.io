---
title: "[Machine Learning with Julia] Independent Component Analysis"
date: 2020-01-23
tags: [machine learning, data science, ica]

excerpt: "Machine Learning, ica, Data Science"
mathjax: "true"
---

# Independent Component Analysis & ICA Factorization

## 1. Independent component analysis

Consider the manifold optimization problem

$$ x_{\sf opt} = \textrm{arg max} |\kappa_4(x^T A)| ,$$

subject to the spherical constraint

$$|| x \parallel_{2}^{2} = 1,$$

where, for  $$y \in R^n ,  K_4(y^T)$$  is the fourth central cumulant or kurtosis and is defined as

$$\kappa_4(y^T) = \dfrac{1}{n} \sum_i y_i^4 - 3 \left[ \dfrac{1}{n} \sum_i y_i^2 \right]^2.$$

The kurtosis code is following :

```julia
kurtosis(x::Vector, A::AbstractMatrix) = mean(x -> x^4, x' * A) - 3 * mean(x -> x^2, x' * A)^2
```

> Thus while PCA finds directions that maximize the variance, ICA will find directions that maximize the absolute kurtosis.

## 1.1. Cumulant

The cumulants of a random variable are a set of quantities that capture information about a random variables. They are closely related to the moments and the central moments of a random variable. The first six cumulants are related to the central moments via the relationships:

$$\begin{align*}
\kappa_{1} &= \mu_1' \\
\kappa_{2}&=\mu_{2} \\
\kappa_{3}&=\mu_{3} \\
\kappa_{4}&=\mu_{4}-3\,{\mu_{2}}^{2} \\
\kappa_{5}&=\mu_{5}-10\,\mu_{3}\mu_{2}\\\
\kappa_{6}&=\mu_{6}-15\,\mu_{4}\mu_{2}-10\,{\mu_{3}}^{2}+30\,{\mu_{2}}^{3}
\end{align*}
$$

The following code is about a function called kappa that takes as inputs a vector x and an integer order and returns the empirical cumulant of the corresponding order.

```julia
function kappa(x::AbstractArray, order::Int)
    if (order < 1 || order > 6)
        error("Order out of defined bounds")
    end

    xc = x .- mean(x) # Centered data
    mu(k) = mean(xc.^k) # use this function below

    if order == 1 # First cumulant
        res = mean(x) # Mean
    elseif order == 2 # Second cumulant
        res = mu(2) # Variance
    elseif order == 3 # Third cumulant
        res = mu(3) # Third central moment
    elseif order == 4 # Fourth cumulant
        res = mu(4) - 3*mu(2)^2
    elseif order == 5 # Fifth cumulant
        res = mu(5)-10*mu(3)*mu(2)
    elseif order == 6 # Sixth cumulant
        res = mu(6)-15*mu(4)*mu(2)-10*mu(3)^2+30*mu(2)^3    
    end

    return res
end
```

## 1.2. Independent component analysis

Now we consider the ICA optimization problem involving the absolute kurtosis maximizing directions


$$x_{\sf opt} = \arg \max  |\,\kappa_4(x^T A)\,|  \qquad \textrm { subject to } ||x\parallel_{2} = 1,$$

where we are optimizing over the spherical manifold and where, for $$y \in \mathbb{R}^{n} , \kappa_4(y^T)$$ is the fourth order cumulant.

The following code contains the manifold optimization implementations.

```julia
function optimq1(A, maxiters=10000, x0::Vector = randn(size(A, 1)); obj::Function=absk4)
    m, n = size(A)

    x0 = normalize(x0) # to make it unit norm

    opt = Optim.optimize(
        x -> -obj(x'A), x0,      
        Optim.ConjugateGradient(
            manifold=Optim.Sphere()
            ),
        Optim.Options(iterations=maxiters)
    )
    xopt = opt.minimizer
    return xopt
end
```

## 1.3. Proving that ICA can unmix "white" independent variables while PCA cannot

Indeed via the above formulation, we can think of PCA and ICA via the same cumulant maximization framework.

PCA finds the directions that maximize  $$K_2$$ whereas ICA has to do with maximization of higher order cumulants. In that sense there are many ICAs associated with whatever cumulant is being maximized.

We begin with the variant of ICA that maximizes  $$\left | k_4 \right |$$ and show that ICA, via kurtosis maximization, does indeed recover the independent components.

Now we contrast $$ w_{pca}$$ and $$w_{ICA}$$ where

$$w_{\sf pca} = \arg \max \kappa_2(w_1 X + w_2 Y) \textrm { subject to } w_1^2 + w_2^2 = 1$$

and

$$w_{\sf ica} = \arg \max |\kappa_4(w_1 X + w_2 Y)| \textrm { subject to } w_1^2 + w_2^2 = 1$$

for the setting that $$X$$ and $$Y$$ are independent variables.

We will show that ICA will succeed to recover by $$w_{\sf ica} = \pm e_1$$ or $$w_{\sf ica} = \pm e_2$$.

Therefore, when we mix two independent random variable having the same variance($$K_2$$) then we cannot find the direction by finding the direction that maximize the variance(PCA method) because all of direction is equally good.

What happens when we maximize  $$\left | k_4 \right |$$  or equivalently use abs. kurtosis based ICA?

## 1.4. Proving that ICA unmixes orthogonally mixed white independent variables.

This is the basic idea we will use when trying to understand why ICA succeeds in unmixing mixed independent random variables. To that end, we now mix the variables together via the model:

$$Y = Qx,$$

where  $$Q$$  is an orthogonal matrix and  $$x$$  is a zero mean, "white" vector with independent components. The whiteness assumption implies that  $$x$$  has an identity covariance matrix

A consequence of this assumption is that

$$\kappa_2(w^T y)  = 1 ,$$

for every unit norm vector  $$w$$ . Thus  $$K_2$$  maximization would not find any interesting direction and so PCA would fail to unmix the variables.

Instead, we consider the abs. kurtosis based ICA optimization problem:

$$w_{\sf opt} = \arg \max_{||w\parallel_{2} = 1} | \kappa_4 (w^T y)|,$$

We will show that  $$W_{opt}$$  thus obtained will unmix the independent random variables from  $$y$$ .

**Step1**: Change of variables
We first reformulate the optimization problem by a change of variables. We set
$$\tilde{w} = Q^T w.$$

**Step2**: We now rewrite the abs. kurtosis maximization problem in terms of $$\tilde{w}$$.

$$K_4(\tilde{w}^TQ^TQx)$$ since $$y = Qx$$.

**Step3**: Via the change of variables we obtain the equivalent optimization problem:

$$\tilde{w}_{\sf opt} = \arg \max | \kappa_4 (\tilde{w}^T x )| \qquad {||\tilde{w}\parallel_{2} = 1},$$

from which we can compute the desired  $$w_{opt}$$  via

$$w_{\sf opt} = Q  \tilde{w}_{\sf opt}.$$

Thus, if we can theoretically understand  $$\tilde{w}_{\sf opt}$$  we will have a deep understanding of how (and whether) ICA is able to unmix mixed independent random variables.

**Step4**: By using the properties of cumulants, as we did in the setting where there were two random variables  $$X$$  and  $$Y$$ , we can establish an upper bound on the ICA objective function and show that
$$|\kappa_4(\tilde{w}^T x)| \leq \sum_{i=1}^{n} w_i^4 |\kappa_4(x_i)| \leq 1 \cdot \max_i |\kappa_4(x_i)|$$

**Step5**: Determine elements that attain the upper bound, as described next.

$$\pm e_i$$ for $$i = \arg \max_j |k_4(x_j)|$$

**Step6**: Obtain from  $$w_{\sf opt}$$  from  $$\tilde{w}_{\sf opt}$$ via change of variables.

$$w_{\sf opt} = Q\tilde{w}_{\sf opt}$$

**Recap**
We have established that when
$$y = Q x,$$
and  $$Q$$  is an orthogonal matrix and the elements of  $$x$$  are independent, zero-mean "white" random variables, then

$$w_{\sf opt} = \arg \max_{||w\parallel_{2} = 1} | \kappa_4 (w^T y)|,$$

yields  $$w_{\sf opt}$$  which equals  $$\pm$$  the column of  $$Q$$  corresponding to the element of  $$x$$  with largest absolute kurtosis.

Note that when multiple variables have the same absolute kurtosis as any of the corresponding columns of  $$Q$$  is a valid optimum but, unlike PCA, not a linear combination of them.

Having estimated $$w_{\sf opt}$$ thus, we note that

$$w_{\sf opt}^T y = \pm x_i ,$$

and thereby unmixing one of the independent components from their mixture  $$y$$.

## 1.5. What independent random variables can ICA not unmix?

> ICA Identifiability condition: ICA can umix random variables provided no more than one of them is Gaussian.

When there is more than one Gaussian in the mixture, the kurtosis and higher order cumulants cannot unmix the Gaussian components because the cumulants of a Gaussian random variable are identically zero.

## 2. Independent Component Factorization

We now obtain all the independent components one component at a time by solving the optimization problem

$$q_i = \arg \max_w |\kappa_4 (w^T y)|,$$

subject to $$\left \| w \parallel_{2} = 1$$ and $$w \perp q_1, \ldots, q_{i-1}$$

This is a constraint where  $$w$$  lies on the intersection of the sphere and the complement of the subspace spanned by  $$q_1,\ldots, q_{i-1}$$.

The function optimQ computes the orthogonal matrix  $$Q$$  whose columns  $$q_i$$  are solutions to the ICA manifold optimization problem.

```julia
absk4(y) = abs(kappa(y, 4))

function optimQ(
        A::Matrix,
        k::Integer=minimum(size(A)),
        maxiters=1000,
        x0=normalize(randn(size(A, 1)));
        obj=absk4
    )

    m, n = size(A)
    q1 = optimq1(A, maxiters, x0; obj=obj)
    Q  = reshape(q1, :, 1)
    opt_obj_values = obj(q1' * A)

    for i in 2:min(k, m - 1)
        P_ortho_Q = I - Q * Q'
        xopt = optimq1(P_ortho_Q * A, maxiters, x0; obj=obj)
        opt_obj_values = vcat(opt_obj_values, obj(xopt' * A))
        Q = hcat(Q, xopt)
    end

    if k == m
        qm = (I - Q * Q') * randn(m)
        qm = qm / norm(qm)
        opt_obj_values = vcat(opt_obj_values, obj(qm' * A))
        Q = hcat(Q, qm)
    end

    ## Sort the components in descending order with respect to the objective function
    obj_sorted = sortperm(opt_obj_values, rev=true)
    Q = Q[:, obj_sorted]

    return Q
end
```

## 2.1. Factorization

We showed earlier that ICA can be used to unmix a mixture of the form  $$Y = Qx$$  where $$Q$$  is orthogonal. We now consider the general setting

$$ y = Ax, $$

where, as before,  ð‘¥  has an identity covariance matrix but  ð´  need not be an orthogonal matrix.

Our goal is to understand how ICA can be applied here to unmix the independent random variables.

Let  $$A = U \Sigma V^T$$  be the SVD of the matrix  $$m\times n$$ matrix  $$A$$ .

Then,

$$y = U \Sigma V^T x,$$

Define

$$\tilde{y} = \Sigma^{+} U^T y.$$

Then we have that

$$\tilde{y} = V^{T} x.$$

If  $$r = n \leq m$$  then  $$V$$  is an orthogonal matrix so we are back to the same setup as before where the orthogonal mixing matrix! Thus we can use manifold optimization on  $$\tilde{y}$$   to find the  $$V$$  and unmix the variables.

We would like to factorize a data matrix into a matrix of the form

$$Y = W_{\sf ica} X_{\sf ica},$$

where the rows of  $$X_{ica}$$  are independent (or at least "as independent as possible"). This is in contrast to PCA where the analogous  $$X$$  has rows that are uncorrelated rather than independent.

Note that  $$X_{ica}$$  is not unique because for an arbitrary diagonal matrix  $$D$$  we can write

$$Y = (W_{\sf ica} D) (D^{-1} X_{\sf ica}).$$

We will choose a normalization so that  $$X_{ica}$$  has an (proportional to) identity covariance matrix.

Motivated by our insights above, we will first express  $$Y$$  as

$$Y = \mu_Y 1^T + U \Sigma V^T,$$

where $$U \Sigma V^T$$ is the SVD of $$Y - \mu_Y 1^T$$ and $$\mu_Y$$ is the mean of column vector of $$Y$$.

By construction, the sample covariance matrix of `sqrt{size(V',2))*V'` is the Identity matrix. So we an apply ICA to the rows of the rescaled  $$V^T$$  to find  $$Q_{ica}$$  using the optimQ function. This yields the decomposition

$$V^T = Q_{\sf ica} V_{\sf ica}^T,$$

where

$$V_{\sf ica}^T = Q_{\sf ica}^T V^T.$$

Plugging the expression for  $$V^T$$  back into the expression for  $$Y$$  gives us

$$Y = \mu_Y 1^T + U \Sigma Q_{\sf ica} V_{\sf ica}^T.$$

or equivalently

$$Y = U \Sigma Q_{\sf ica} \left[(U \Sigma Q_{\sf ica})^{-1} (\mu_A 1^T) + V_{\sf ica}^T  \right].$$

Define

$$W_{\sf ica} = U \Sigma Q_{\sf ica},$$

and

$$X{\sf ica} = \left[(U \Sigma Q_{\sf ica})^{-1} (\mu_A\, 1^T) + V_{\sf ica}^T  \right] = W_{\sf ica}^{-1} \mu_A 1^T + V_{\sf ica}^T,$$

then we have that

$$Y = W_{\sf ica} X_{\sf ica},$$

as desired. This is the ICA factorization of a matrix.

```julia
function ica_factorization(Y::AbstractArray, k::Integer=minimum(size(Y)), maxiters::Integer=1000)
    m, n = size(Y)
    Î¼y = mean(Y; dims=2)
    Ymean = Î¼y * ones(1, size(Y, 2))
    Ytil = Y - Ymean

    UsV = svd(Ytil)
    U = UsV.U[:, 1:k]
    S = UsV.S[1:k] |> Diagonal
    V = UsV.V[:, 1:k]

    Qica = optimQ(sqrt(size(V, 1)) * V', k, maxiters)
    Vica = V*Qica  ## TODO: Fill in formula for Vica'
    Wica = U*S*Qica  ## TODO: Fill in formula (1) for Wica
    Xica = pinv(Wica) * Ymean + Vica'
    return Wica, Xica, Qica
end
```

Let us compare this ICA factorization with PCA factorization on a dataset.

```julia
function pca_factorization(Y, k=size(Y, 1))
    Î¼y = mean(Y; dims=2)
    Ymean = Î¼y * ones(1, size(Y, 2))
    Ytil = Y - Ymean

    UsV = svd(Ytil)
    U = UsV.U[:, 1:k]
    S = UsV.S[1:k] |> Diagonal
    V = UsV.V[:, 1:k]

    Wpca = U * S
    Winv = inv(S) * U'
    Xpca = Winv * Ymean + V'

    return Wpca, Xpca
end
```
Let's consider random matrix A.

```julia
A = [1 0.5; 0.25 0.75]
U, s, V = svd(A)
X = rand(2, 1000)
Y = A * X
```

```julia
Wica, Xica, Qica = ica_factorization(Y, 2)
Xpca = pca_factorization(Y,2)[2];
```

```julia
pointcloud(A::Matrix; kwargs...) = scatter(A[1, :], A[2, :]; alpha=0.5, aspect_ratio=:equal, kwargs...)
py = pointcloud(Y; title="Y")
pica = pointcloud(Xica; title=" ICA coordinates")
ppca = pointcloud(Xpca; title=" PCA coordinates")
plot(py, pica, ppca; layout=(1, 3), size=(850, 300))
```

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/ica_pca.png" alt="">

> ICA produces coordinates that are (approximately) independent relative to PCA
