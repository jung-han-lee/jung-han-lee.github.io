---
title: "[Machine Learning with Julia] Principal Components Analysis"
date: 2020-01-21
tags: [machine learning, data science, pca]

excerpt: "Machine Learning, pca, Data Science"
mathjax: "true"
---

# Principal Components Analysis

## 1. Matrix Factorization

We need to know matrix factorization to understand the principal components analysis.

Let  $$A$$  be an  $$m \times n$$  matrix. The range or column space of  $$A$$  denoted by  $$\mathcal{R}(A)$$  is the subspace of vectors formed from all linear combinations of its columns  $$A[:,1],â€¦,A[:,n]$$ . Let  $$r$$  denote the rank of  $$A$$ , i.e., the number of linearly independent columns.

We would like to find $$r$$ **basis vectors $$w_1, 2_2, ... , w_r$$** for $$\mathcal{R}(A)$$ and express each column of $$A$$ as a linear combination with respect to theses bases vectors.

For instance, the first column can be expressed as

$$A[:,1] = w_{1} x_{11} + \ldots w_{r} x_{r1},$$

and the second column as

$$A[:,2] = w_{1} x_{12} + \ldots w_{r} x_{r2},$$

and so on.

This yields the **matrix factorization**

$$A = WX,$$

where $$X$$ is an $$r\times n$$ matrix and

$$A[:,j] = w_{1} x_{1j} + \ldots w_{r} x_{rj}.$$

## 2. The largest left and right singular vector
The left and right singular vectors can be learned from  $$A$$ via an optimization problem.

## 2.1. The largest right singular vector

Let's consider a **best direction** problem that is formulated as follows :

$$\begin{equation}\label{eq:manopt1}
x_{\sf opt} = \textrm{arg max}_{||x\parallel_{2} = 1} \parallel A x \parallel_{2},
\end{equation}$$

This problem can be solved via singular value decomposition. Let the SVD of $$A$$ be given by
$$A = \sum_{i=1}^{r} \sigma_{i} u_i v_i^T,$$
where $$\sigma_{1} \geq \ldots \geq \sigma_{r}$$ are it's singular values. Then

$$x_{\sf opt} = v_{1}.$$

In other words, $$x_{opt}$$ is the right singular vector of $$A$$ associated with its largest singular value.

Many numerical optimization packages _minimize_ functions. So to use them, we will have to recast the problem of finding $$u_1$$ as a minimization problem instead of a maximization problem.

Therefore, we will define the function xhatMaximizes_norm_Ax_onSphere which takes as its input the matrix A and returns as its output xopt.

```julia
function xthatMaximizes_norm_Ax_onSphere(
        A::AbstractMatrix,
        maxiters::Integer=1000,
        x0::Vector = randn(size(A,2))
    )
    m, n = size(A)
    x0 = normalize(x0) ## make it unit norm

    opt = Optim.optimize(
        x -> -norm(A * x), x0,      ## TODO: Add the objective function
        Optim.ConjugateGradient(
            manifold = Optim.Sphere()
            ),
        Optim.Options(iterations = maxiters)
    )

    xopt = opt.minimizer

    return xopt
end
```

## 2.2. The largest left singular vector

Since

$$||x^T A||2 = ||A^T x||2,$$

and given that if $$A = U \Sigma V^T$$, then $$A^T = V\, \Sigma^T U^T$$, we can conclude that

$$ \begin{equation}
u_1(A) = \arg \max_{||x||2 = 1} ||x^T A\parallel_{2}  
\end{equation}$$

## 2.3. Learning the right singular vectors one at a time

Now let's solve the optimization problem

$$x_{\sf opt} = \arg \max_{x} \parallel A\, x \parallel_{2}$$

subject to the spherical constraint

$$||x\parallel_{2}^{2} =  1,$$

and the orthogonality constraint $$x \perp v_{1}, \ldots v_{k-1}$$ with $$v_0=0$$.

It can be shown that

$$ x_{opt} = v_k.$$

This provides a recipe for peeling off the singular vectors one at a time. We need to optimize over the set of vectors with unit norm that are orthogonal to the subspace spanned by the columns of the matrix

$$V = \begin{bmatrix} v_{1} & \ldots & v_{k-1}
\end{bmatrix}$$

The function manoptV is following

```julia
function manoptV(A::AbstractMatrix, k::Integer=minimum(size(A)), maxiters::Integer=1000)

    m, n = size(A)
    xopt = xthatMaximizes_norm_Ax_onSphere(A, maxiters)
    V = xopt

    for i in 2 : min(k, n - 1)
        P_ortho_V = I - V * V'
        xopt = xthatMaximizes_norm_Ax_onSphere(A * P_ortho_V, maxiters)
        V = hcat(V, xopt)
    end

    if k == n
        vn = (I - V * V') * randn(n)
        vn = vn / norm(vn)
        V = hcat(V, vn)
    end
    return V
end
```
## 3. Principal Component Analysis & the PCA matrix Factorization

Let $$A$$ ne an $$m \times n$$ matrix whose columns are $$A[:,1],...,A[:,n]. Let $$x$$ be an $$ m\times 1$$ unit norm vector.

**Step1** : Note that the elements of the row vectors
$$ x^T A = \begin{bmatrix} x^T A[:,1], & x^T A[:,2], & \ldots & x^T A[:,n] \end{bmatrix}, $$
can be interpreted as the coordinates of the columns of A with respect to the basis vector $$x$$.

**Step2**: Then, the variance of the coordinates is given by

$$\textrm{var}(x^T A) =  \frac{1}{n}\sum_{i=1}^{n} \left(x^T A[:,i] -\frac{1}{n}\sum_{j} x^T A[:,j]\right)^2,$$

which can be rewritten as

$$
 \textrm{var}(x^T A) = \frac{1}{n} \sum_{i=1}^{n} \left[x^T \left(A[:,i] -\frac{1}{n}\sum_{j} A[:,j]\right) \right]^2.$$

 **Step3**: Change of variables.

 Let

 $$\overline{\mu}_A = \frac{1}{n} \sum_{i} A[:,i] = \frac{1}{n} A\,\mathbf{1},$$

 Then, we have that
 $$\textrm{var}(x^T A) =  \frac{1}{n} \sum_{i=1}^{n} \left[x^T \left(A[:,i] -\overline{\mu}_A\right)\right]^2.$$

 Let the centered data matrix $$\overline{A}$$ as

 $$\overline{A} = A - \overline{\mu}_A \mathbf{1}^T = A - \frac{1}{n} A \mathbf{1}\mathbf{1}^T.$$

 Then the variance along the direction $$x$$ is given by

 $$
\textrm{var}(x^T A) = \frac{1}{n} \parallel x^T \overline{A} \parallel_2^{2}.$$

**Step4**: Putting it together, we see that the problem of finding the  "direction that maximizes variance" can be cast as the optimization problem

$$
x_{\sf opt} = \arg \max \textrm{var}(x^T A) =  \frac{1}{n} \arg \max \parallel x^T \overline{A} \parallel_2^{2},
$$

subject to $$\parallel x \parallel_2 = 1.$$

This direction is the so-called Principal Component, and applying what we derived earlier, we have that

$$x_{\sf opt} = u_{1}(\overline{A}).$$

Using the defined function **manoptV**, we can define a function **manoptPCs** which computes the $$k$$ leading principal components.

```julia
function manoptPCs(A::AbstractMatrix, k::Integer, maxiters::Integer=1000)
     m, n = size(A)
     centeredA = A .- mean(A, dims = 2)
    return manoptV(centeredA', k, maxiters)
end
```

## 3.1. PCA factorization of a matrix

Now, we wish to express the matrix $$Y$$ as $$Y = W_{pca}X_{pca}$$ where the $$W_{pca} is the orthogonal(or unitary) matrix whose columns are the principal component directions.

**Step1**:

We first express  $$Y$$  as the sum of a mean matrix  $$\overline{Y}$$  and the matrix  $$\widetilde{Y}$$  whose columns have mean zero such that

$$Y = \overline{Y} + \widetilde{Y},$$

where

$$\overline{Y} = \mu_Y \mathbf{1}^T,$$

and

$$\mu_Y = \dfrac{1}{n} \sum_{i=1}^{n} Y[:,i].$$

**Step2**:

We then express $$\widetilde{Y}$$ via its SVD as

$$\widetilde{Y} = U \Sigma V^H.$$

**Step3**:

With the $$U$$ thus returned by PCA, we can decompose

$$Y=W_{\sf pca} X_{\sf pca},$$

The function **pca_factorization** which takes as its input the matrix $$Y$$ and returns as its output $$W_{pca}$$ and $$X_{pca}$$.

```julia
function pca_factorization(Y::AbstractMatrix)
    y_mean = mean(Y; dims=2)
    Ymean = y_mean * ones(1, size(Y, 2))
    Ytil = Y - Ymean

    U, s, V = svd(Ytil, full=false)

    Wpca = U*Diagonal(s)
    Winv = inv(Wpca)
    Xpca = Winv * Ymean + V'

    return Wpca, Xpca
end
```

## 4. Application

We now illustrate how the PCA factorization of a data matrix can provide interesting insights on the data, via the principal coordinates.

Let's load a dataset.

```julia
using CSV, DataFrames
Xdata = CSV.read("breast-cancer-wisconsin.data.txt")
```

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/data1.png" alt="">

The columns 2-10 of this dataset contain the data we will analyze. Column 11 contains label information (whether a cell was "benign" or "malignant") that we will not use in the analysis but will use in the visualization of the analyzed data. To that end, we create a  $$9\times 699$$  matrix matrix A_breastcancer whose rows represent the various features and the columns represent different samples (associated with different patients). Some of the samples correspond to benign cells while others have malignant cells and so we will extract this label information for use later as below.

```julia
A_breastcancer = Matrix(Xdata[:,2:10])'
benign = Xdata[:,end] .== 2
println("Number of bening samples = $(sum(benign))");
malignant = Xdata[:,end] .== 4;
println("Number of malignant samples = $(sum(malignant))");
Xdata[:,end]
```

Thus projecting the 9-dimensional data onto the canonical Euclidean coordinates corresponding to idx1 and idx2 does not yield any insights.

We now consider the PCA factorization of the same matrix and generate a scatter plot using the first and second principal coordinates.

```julia
Wpca, Xpca = pca_factorization(A_breastcancer)
scatter(Xpca[1,benign],Xpca[2,benign],
    color=:red, marker=:square, label = "benign", legend=:bottomright,
    xlabel = "PCA Coordinate 1", ylabel = "PCA Coordinate 2")
p_pca = scatter!(Xpca[1,malignant],Xpca[2,malignant], color=:blue, label = "malignant")
plot!(; title = "PCA")
```

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/pca_plot.png" alt="">

> Thus, PCA factorization provides insights that we cannot get from the dataset.
