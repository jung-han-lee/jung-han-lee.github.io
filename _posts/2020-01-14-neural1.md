---
title: "Machine Learning Project: Neural Networks 1"
date: 2020-01-14
tags: [machine learning, data science, neural network]

excerpt: "Machine Learning, Neural Network, Data Science"
---


#Machine Learning : Neural Network

##1. Neural Networks : Neurons and Activation Functions

A neural network is a network of neurons connected together.

Each neuron in a neural network has associated with it a scalar-valued _activation function_ $f_a(\cdot)$, along with a weight $w$ and a bias $b$.

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/single.png" alt="">


The neuron represented in the figure above takes as its input a scalar  ùë• , and returns as the output

$$f_1(x,\color{green}{w},\color{blue}{b}) = f_{a}(\color{green}{w}\,\color{black}x + \color{blue}{b}).$$

Here $w$ is the weight of the input _edge_ to the neuron.

###1.1 Linear networks

Let's begin with the simplest, and the most boring activation function. This is the linear activation function defined mathematically by the function

$$\textrm{linear}(z)=z.$$

We can define this in Julia as in the next cell.

`Linear function`

```Julia
linear(z::Number) = z
```

We also need the derivative of the linear activation function.

`Derivative of linear function`

```Julia
dlinear(z::Number) = 1
```


###1.2 Non-linear networks

Non-linear networks use nonlinear activation functions.

A popular nonlinear activation function is the hyperbolic tangent. We do need to define its derivative which is mathematically given by

$$\partial_z \tanh(z) = 1-\tanh^2(z)$$

`Derivative of hyperbolic tangent function`

```Julia
dtanh(z) = 1 - tanh(z)^2
```


###1.3 Representing the output of a single neuron to a stream of inputs
