---
title: "Machine Learning with Julia: Neural Networks & Loss Functions"
date: 2020-01-14
tags: [machine learning, data science, neural network, activation function, loss function]

excerpt: "Machine Learning, Neural Network, Data Science"
mathjax: "true"
---


# Machine Learning : Neural Network

## 1. Neural Networks : Neurons and Activation Functions

A neural network is a network of neurons connected together.

Each neuron in a neural network has associated with it a scalar-valued _activation function_ $$ f_a(\cdot) $$, along with a weight $$ w $$ and a bias $$ b $$.

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/single.png" alt="">


The neuron represented in the figure above takes as its input a scalar  ùë• , and returns as the output

$$ f_1(x,\color{green}{w},\color{blue}{b}\color{black}) = f_{a}(\color{green}{w}\,\color{black}x + \color{blue}{b}). $$

Here $$w$$ is the weight of the input _edge_ to the neuron.

### 1.1 Linear networks

Let's begin with the simplest, and the most boring activation function. This is the linear activation function defined mathematically by the function

$$ \textrm{linear}(z)=z. $$

We can define this in Julia as in the next cell.

```julia
linear(z::Number) = z
```

We also need the derivative of the linear activation function.

```julia
dlinear(z::Number) = 1
```


### 1.2 Non-linear networks

Non-linear networks use nonlinear activation functions.

A popular nonlinear activation function is the hyperbolic tangent. We do need to define its derivative which is mathematically given by

$$\partial_z \tanh(z) = 1-\tanh^2(z)$$


```julia
dtanh(z) = 1 - tanh(z)^2
```


### 1.3 Representing the output of a single neuron to a stream of inputs

We would now like to mathematically represent the output of a single neuron to a stream of inputs.

If $$ X $$ is an $$ 1 \times n $$ array with

$$ X = \begin{bmatrix} X[1] & X[2] & \cdots &  X[n] \end{bmatrix}, $$

then

$$ f.(X) = \begin{bmatrix} f(X[1]) & f(X[2]) & \cdots &  f(X[n]) \end{bmatrix}. $$

We define  $$f_1(x,w,b)$$ , using the . operator.

```julia
f‚ÇÅ(x, w, b, f_a) = f_a.(w * x .+ b)
```

## 2. A simple neural network that learns to classify

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/mul_input.png" alt="">

Now, let's examine a neural network with $n$ input neurons having weights $$ w_1, ... ,w_n $$ associated with inputs $$x_1, ... ,x_n $$. The output of this network is given by

$$\begin{equation}
g_n(x,w,b) = f_{\rm a}\left(\sum_{i=1}^{n} w_i x_i +b\right),
\end{equation}$$

where weight vector $$w$$ and the vector of input $$ x $$ are

$$w = \begin{bmatrix} w_1 & \ldots w_n \end{bmatrix}^T,$$
$$x = \begin{bmatrix} x_1 & \ldots x_n \end{bmatrix}^T.$$

Then we can express the output as

$$\begin{equation}
g_n(x,w,b) = f_{\rm a}( x^T w + b)~.
\end{equation}$$

The following code defines the function **gn** which takes as inputs
- the $$ n \times N$$ array $$x$$
- the vector $$w$$ and scalar $$b$$, and
- the activation function $$f_a$$


```julia
function gn(x::Array, w::Vector, b::Number, f_a::Function)
    return f_a.(x' * w .+ b)  
end
```

### 2.1 The mean-squared error loss function

One of the most commonly used loss function is **mean-squared-error:**

$$L(x,y,w,b) = \dfrac{1}{N} \sum_{j=1}^{N} \left[y_j - f_{\rm a}\left(\sum_{i=1}^{n} w_i x_{ij} + b\right)\right]^2,$$

Optimal weights and biases are those that $$minimize L$$ . With optimal choices of  $$w$$  and  $$b$$ , we say the network is trained. And a trained network should be able to output a value  $$f_{\rm a}\left(\sum_{i=1}^{n} w_i x_{i} + b\right)$$  which is close to the appropriate encoding value  $$y$$  for an input  $$x$$ .

### 2.2 Gradient of the loss function

So the next question is: how do we compute optimal  ùë§  and  ùëè ? The answer comes from calculus.

$$\nabla_{w_k} L(x,y,w,b)  = - \dfrac{2}{N}\sum_{j=1}^{N} \left[y_j -  f_{\rm a}\left(\sum_{i=1}^{n} w_i x_{ij} + b\right)\right] f'_{\rm a}\left(\sum_{i=1}^{n} w_i x_{ij} + b\right) \, x_{kj},$$

and

$$\nabla_{b} L(x,y,w,b)  = - \dfrac{2}{N}\sum_{j=1}^{N} \left[y_j -  f_{\rm a}\left(\sum_{i=1}^{n} w_i x_{ij} + b\right)\right] f'_{\rm a}\left(\sum_{i=1}^{n} w_i x_{ij} + b\right) \, .$$

Let's write a function called loss_gradient which takes as its inputs $$x, y, w, b, f_a,$$ and $$df_a$$ (and optional normalize like in the prelecture), and returns three things: the gradient vector  $$\nabla_{w} L(x,y,w,b)$$, the gradient  $$\nabla_{b} L(x,y,w,b)$$ , and the loss  $$L(x,y,w,b)$$ .

```julia
function grad_loss(
        f_a::Function,
        df_a::Function,
        x::Matrix,
        y::Vector,
        w::Vector,
        b::Number,
        normalize::Bool=true
    )
    dw = zeros(length(w))
    db = 0.0
    loss = 0.0
    for j in 1:size(x, 2)
        error =  y[j] - f_a.(w' * x[:, j] + b) ## TODO: fill in ??
        common_term = (error .* df_a.(w' * x[:, j] + b))
        dw = dw .- 2 * common_term .* x[:,j] ## TODO: fill in ?? Ignore the 1/N part that is done below
        db = db .- 2 * common_term * 1 ## TODO: fill in ?? Ignore the 1/N part that is done below
        loss += error^2
     end

     if normalize
        dw = dw / length(y)
        db = db / length(y)
        loss = loss / length(y)
    end
    return dw, db, loss
 end
 ```

### 2.3 Learning network parameters using gradient descent

Now that we can compute the loss function and its gradient, we are ready to minimize  $$L$$  using gradient descent.

$$w_{k+1} =w_{k} - \mu \,\nabla_w L(x,y,w_k,b_k).$$

$$b_{k+1} =b_{k} - \mu \,\nabla_b L(x,y,w_k,b_k).$$

Here  $$\mu > 0$$  is the learning rate (or step size). The subscript  $$k$$  refers to the value of a variable after  $$k$$  steps or iterations.

```julia
function learn2classify_gd(
      f_a,
      df_a,
      grad_loss,
      x,
      y,
      mu=1e-3,
      iters=500,
      show_loss=true,
      normalize=true,
      seed=1
  )
  n = size(x, 1)

  if seed == false
      w = zeros(n) # vector of zeros
      b = 0.0
  else
      seed!(seed) #initialize random number generator
      w = randn(n)
      b = rand()
  end

  loss = zeros(iters)
  for i in 1:iters

      dw, db, loss_i = grad_loss(f_a, df_a, x, y, w, b, normalize)
      w = w - mu * dw
      b = b - mu * db
      loss[i] = convert(Float64, loss_i[1])

      if show_loss
          if(rem(i,100) == 0)
              IJulia.clear_output(true)
              loss_plot = scatter(
                  [1:50:i], loss[1:50:i], yscale=:log10,
                  xlabel="iteration",
                  ylabel="training loss",
                  title="iteration $i, loss = $loss_i"
              )
              display(loss_plot)
#                 sleep(0.1)
          end
      end
  end
  return w, b, loss
end
```

### 2.4 Faster learning with stochastic gradient descent

So far we have evaluated the gradient and loss function over the entire training set. This can be slow if there are many training samples; the training dataset may even be too large to fit in the memory of a single machine. Stochastic Gradient Descent (SGD) addresses this by evaluating the gradient and loss function only for randomly chosen subsets (called "mini batches") of the training data at each iteration.

$$w_{k+1} = w_{k} - \mu \,\nabla_w \textrm{L}({\textrm{ random mini batch of }}{x},{\textrm{ random mini batch of }}{y},w_k,b_k),$$

The function is following

```julia
function learn2classify_sgd(
        f_a,
        df_a,
        grad_loss,
        x,
        y,
        mu=1e-3,
        iters=500,
        batch_size=10,
        show_loss=true,
        normalize=true,
        seed=1
    )

    n = size(x, 1)
    N = size(x, 2)

     if seed == false
        seed!(1)
        w = zeros(n)
        b = 0.0
    else
        seed!(seed)
        w = randn(n)
        b = rand()
    end

    loss = zeros(iters)
    for i in 1:(iters)
        batch_idx = randperm(N)
        batch_idx = batch_idx[1:min(batch_size, N)]
        dw, db, loss_i = grad_loss(f_a, df_a, x[:, batch_idx], y[batch_idx], w, b, normalize)
        w = w - mu * dw
        b = b - mu * db
        loss[i] = convert(Float64, loss_i[1])

        if show_loss == true
            if(rem(i, 100) == 0)
                IJulia.clear_output(true)
                loss_plot = scatter(
                    [1:50:i], loss[1:50:i], yscale=:log10,
                    xlabel="iteration",
                    ylabel="training loss",
                    title="iteration $i, loss = $loss_i"
                )
                display(loss_plot)
#                 sleep(0.1)
            end
        end

    end
    return w, b, loss
end
```

### 2.5 Nesterov's accelerated gradient descent method

Nesterov invented an ingenious method for accelerating gradient descent by modifying the iteration to include a so-called momentum term. This involves computing

$$ q_{k+1} = w_{k} - \mu \,\nabla_w L(\textrm{x},\textrm{y},w_k,b_k),$$

$$w_{k+1} = (1-\gamma_k)\, q_{k+1} + \gamma_k\, q_k.$$

and the scalar parameter  ùõæùëò  is defined as

$$\gamma_k = \dfrac{1-\lambda_k}{\lambda_{k+1}}, \lambda_0 = 0 \textrm{ and } \lambda_{k+1} = \dfrac{1+\sqrt{1+4\,\lambda_{k}^2}}{2}.$$

Accelerated stochastic gradient algorithm (ASGD) function is following(via Julia)

```julia
function learn2classify_asgd(
    f_a,
    df_a,
    grad_loss,
    x,
    y,
    mu=1e-3,
    iters=500,
    batch_size=10,
    show_loss=true,
    normalize=true,
    seed=1
)

n = size(x, 1)
N = size(x, 2)

if seed == false
    b = 0.0
    w = zeros(n)
else
    seed!(seed) # initiliaze random number generator
    w = randn(n)
    b = rand()
end

loss = zeros(iters)

lambdak = 0
qk = w
pk = b
for i in 1:iters
    batch_idx = randperm(N)
    batch_idx = batch_idx[1:min(batch_size, N)]

    dw, db, loss_i = grad_loss(f_a, df_a, x[:, batch_idx], y[batch_idx], w, b, normalize)
    qkp1 = w - mu * dw
    pkp1 = b - mu * db

    lambdakp1 = (1 + sqrt(1 + 4 * lambdak^2)) / 2
    gammak = (1 - lambdak) / lambdakp1

    w = (1 - gammak) * qkp1 + gammak * qk
    b = (1 - gammak) * pkp1 + gammak * pk

    qk = qkp1
    pk = pkp1
    lambdak = lambdakp1

    loss[i] = convert(Float64, loss_i[1])

    if show_loss == true
        if(rem(i,100) == 0)
            IJulia.clear_output(true)
            loss_plot = scatter(
                [1:50:i], loss[1:50:i], yscale=:log10,
                xlabel="iteration",
                ylabel="training loss",
                title="iteration $i, loss = $loss_i"
            )
            display(loss_plot)
#                 sleep(0.1)
        end
    end
end
return w,b, loss
end
```
