---
title: "Machine Learning Project: Neural Networks 1"
date: 2020-01-14
tags: [machine learning, data science, neural network]

excerpt: "Machine Learning, Neural Network, Data Science"
mathjax: "true"
---


# Machine Learning : Neural Network

## 1. Neural Networks : Neurons and Activation Functions

A neural network is a network of neurons connected together.

Each neuron in a neural network has associated with it a scalar-valued _activation function_ $$ f_a(\cdot) $$, along with a weight $$ w $$ and a bias $ b $.

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/single.png" alt="">


The neuron represented in the figure above takes as its input a scalar  ùë• , and returns as the output

$$ f_1(x,\color{green}{w},\color{blue}{b}\color{black}) = f_{a}(\color{green}{w}\,\color{black}x + \color{blue}{b}). $$

Here $w$ is the weight of the input _edge_ to the neuron.

### 1.1 Linear networks

Let's begin with the simplest, and the most boring activation function. This is the linear activation function defined mathematically by the function

$$ \textrm{linear}(z)=z. $$

We can define this in Julia as in the next cell.

`Linear function`

```Julia
    linear(z::Number) = z
```

We also need the derivative of the linear activation function.

`Derivative of linear function`

```Julia
    dlinear(z::Number) = 1
```


### 1.2 Non-linear networks

Non-linear networks use nonlinear activation functions.

A popular nonlinear activation function is the hyperbolic tangent. We do need to define its derivative which is mathematically given by

$$\partial_z \tanh(z) = 1-\tanh^2(z)$$

`Derivative of hyperbolic tangent function`

```Julia
    dtanh(z) = 1 - tanh(z)^2
```


### 1.3 Representing the output of a single neuron to a stream of inputs

We would now like to mathematically represent the output of a single neuron to a stream of inputs.

If $ X $ is an $ 1 \times n $ array with

$$ X = \begin{bmatrix} X[1] & X[2] & \cdots &  X[n] \end{bmatrix}, $$

then

$$ f.(X) = \begin{bmatrix} f(X[1]) & f(X[2]) & \cdots &  f(X[n]) \end{bmatrix}. $$

We define $ f_1(x,w,b) $ using . operator in Julia.

```Julia
    f‚ÇÅ(x, w, b, f_a) = f_a.(w * x .+ b)
```

We can check how does the output of a single neuron changes as we alter the weight $ w $ and the bias $ b $, and how does this depend on the activation function used from the code below.

```Julia
    x = collect(range(-10; stop=10, length=101))
    @manipulate for
      w in (-10, -5, -2.5, 2.5, 5, 10),
      b in (-10, -5, -2.5, 0, 2.5, 5, 10),
      f_a in (tanh, linear)

      # plot x vs f1(x, w, b) for the w and b chosen by buttons:
      plot(
          x, f‚ÇÅ(x, w, b, f_a);
          xlabel="x",
          ylabel="f1(x)",
          title="$f_a activation",
          ylims=(-10, 10)
          )
    end
```

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/plot_single.png" alt="">


## 2. A simple neural network that learns to classify

<img src="{{ site.url }}{{ site.baseurl }}/images/ml1/mul_input.png" alt="">

Now, let's examine a neural network with $n$ input neurons having weights $ w_1, ... ,w_n $ associated with inputs $x_1, ... ,x_n $. The output of this network is given by

$$\begin{equation}
g_n(x,w,b) = f_{\rm a}\left(\sum_{i=1}^{n} w_i x_i +b\right),
\end{equation}$$

where weight vector $w$ and the vector of input $ x$ are

$$w = \begin{bmatrix} w_1 & \ldots w_n \end{bmatrix}^T,$$
$$x = \begin{bmatrix} x_1 & \ldots x_n \end{bmatrix}^T.$$

Then we can express the output as

$$\begin{equation}
g_n(x,w,b) = f_{\rm a}( x^T w + b)~.
\end{equation}$$

The following code defines the function gn which takes as inputs
- the $ n \times N$ array $x$
- the vector $w$ and scalar $b$, and
- the activation function $f_a$


```Julia
    function gn(x::Array, w::Vector, b::Number, f_a::Function)

    return f_a.(x' * w .+ b)  

    end
```

### 2.1 Formulating binary classification as a learning problem

In the binary classification problem, we are given two classes of variables. Let's call them "Class 0" and "Class 1". In the application we will consider, Class "0" will be the digit 0, and Class "1" will be the digit 9.
